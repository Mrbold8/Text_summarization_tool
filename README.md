# –ú–æ–Ω–≥–æ–ª —Ö—ç–ª–Ω–∏–π Text Summarization Model -—É—É–¥

–ú–æ–Ω–≥–æ–ª —Ö—ç–ª –¥—ç—ç—Ä **Text Summarization**, **Title Generation**, **Keywords Extraction** task -—É—É–¥–∞–¥ –∑–æ—Ä–∏—É–ª–∞–Ω fine-tuning —Ö–∏–π—Å—ç–Ω 5 model -–∏–π–≥ —ç–Ω—ç—Ö“Ø“Ø repo -–¥ –æ—Ä—É—É–ª—Å–∞–Ω. Model -—É—É–¥—ã–≥ **PEFT / LoRA** –∞—Ä–≥–∞—á–ª–∞–ª–∞–∞—Ä ”©”©—Ä–∏–π–Ω –±—ç–ª–¥—Å—ç–Ω –º–æ–Ω–≥–æ–ª —Ö—ç–ª–Ω–∏–π dataset –¥—ç—ç—Ä fine-tuning —Ö–∏–π–∂, –≥“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–≥ —Å–∞–π–∂—Ä—É—É–ª—Å–∞–Ω.

---

## üìÑ –°—É–¥–∞–ª–≥–∞–∞–Ω—ã –∞–∂–ª—ã–Ω —Ç–∞–π–ª–∞–Ω

[![–¢–∞–π–ª–∞–Ω–≥ —Ö–∞—Ä–∞—Ö (PDF)](https://github.com/Mrbold8/Text_summarization_tool/blob/main/%D0%A1%D1%83%D0%B4%D0%B0%D0%BB%D0%B3%D0%B0%D0%B0%D0%BD%D1%8B_%D0%B0%D0%B6%D0%B8%D0%BB.pdf)

---

## Dataset

–ú–æ–Ω–≥–æ–ª —Ö—ç–ª–Ω–∏–π –±–∏—á–≤—ç—Ä –±–æ–ª–æ–Ω —Ö—É—Ä–∞–∞–Ω–≥—É–π–≥–∞–∞—Å –±“Ø—Ä–¥—ç—Ö dataset –±–æ–ª–æ–≤—Å—Ä—É—É–ª–∂, CNN/DailyMail dataset -–∏–π–≥ Google Batch Translate –∞—à–∏–≥–ª–∞–Ω –º–æ–Ω–≥–æ–ª —Ö—ç–ª —Ä“Ø“Ø —Ö”©—Ä–≤“Ø“Ø–ª—ç–Ω —Ö—ç–º–∂—ç—ç–≥ –Ω—ç–º—ç–≥–¥“Ø“Ø–ª—Å—ç–Ω. –î–∞—Ä–∞–∞ –Ω—å –≥–∞—Ä—á–∏–≥ –±–æ–ª–æ–Ω —Ç“Ø–ª—Ö“Ø“Ø—Ä “Ø–≥ “Ø“Ø—Å–≥—ç–Ω dataset -—ç—ç –±–∞—è–∂—É—É–ª—Å–∞–Ω.

Dataset —Ö–æ–ª–±–æ–æ—Å—É—É–¥:

- https://huggingface.co/datasets/amaraaa/mn_translated_cnn_extended  
- https://huggingface.co/datasets/amaraaa/multi-task_mn_cnn_v02  

---

## Fine-tuned Model-—É—É–¥

### 1. mT5-small summarization (LoRA)
Encoder‚ÄìDecoder architecture –±“Ø—Ö–∏–π model.  
**RougeLsum:** ~13.8  
**HF:** https://huggingface.co/amaraaa/mt5-small-summarization-mn-v1  

---

### 2. mT5-base summarization (LoRA)
–ò–ª“Ø“Ø —Ç–æ–º model —Ç—É–ª –º–æ–Ω–≥–æ–ª —Ç–µ–∫—Å—Ç–∏–π–Ω —É—Ç–≥–∞, –Ω–∞–π—Ä—É—É–ª–≥—ã–≥ –∏–ª“Ø“Ø —Å–∞–π–Ω –≥–∞—Ä–≥–∞–Ω–∞.  
**RougeLsum:** ~18.57  
**HF:** https://huggingface.co/amaraaa/mt5-base-summarization-mn-v1  

---

### 3. mBART-50-large summarization (LoRA)
–ú–æ–Ω–≥–æ–ª —Ö—ç–ª –¥—ç—ç—Ä —Ö–∞–º–≥–∏–π–Ω ”©–Ω–¥”©—Ä –≥“Ø–π—Ü—ç—Ç–≥—ç–ª “Ø–∑“Ø“Ø–ª—Å—ç–Ω model.  
**RougeLsum Test:** ~31.47  
**HF:** https://huggingface.co/amaraaa/mbart50-mn-summarization_v2  

---

### 4. mongolian-gpt2 summarization (LoRA)
Decoder-only architecture –±“Ø—Ö–∏–π —Ç—É—Ä—à–∏–ª—Ç—ã–Ω summarization model.  
**HF:** https://huggingface.co/amaraaa/gpt2-summarization-mn-v1  

---

### 5. Multitask mT5-base (Summarization + Title + Keywords)
Summarization, Title generation, Keywords extraction task -—É—É–¥—ã–≥ –Ω—ç–≥ model –¥—ç—ç—Ä –∑—ç—Ä—ç–≥ —Å—É—Ä–≥–∞–∂ fine-tuning —Ö–∏–π—Å—ç–Ω —Ö—É–≤–∏–ª–±–∞—Ä.  
**HF:** https://huggingface.co/amaraaa/multitask-mt5-base-mn-v1  

---

## –ê—à–∏–≥–ª–∞—Å–∞–Ω —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏

- HuggingFace, Google Colab
- Transformers, Datasets, PEFT-LoRA  
- Google Batch Translate  
- Hugging Face Inference Endpoint  
- Next.js, React, TypeScript  
- AWS Deployment  

---

## –¢–æ–≤—á –¥“Ø–≥–Ω—ç–ª—Ç

Model -—É—É–¥–∞–∞—Å **facebook/mBART-50-large** –±–æ–ª–æ–Ω **google/mT5-Base** –ú–æ–Ω–≥–æ–ª —Ö—ç–ª –¥—ç—ç—Ä —Ö–∞–º–≥–∏–π–Ω ”©–Ω–¥”©—Ä —á–∞–Ω–∞—Ä—Ç–∞–π —Ö—É—Ä–∞–∞–Ω–≥—É–π “Ø“Ø—Å–≥—ç—Å—ç–Ω. –ë—É—Å–∞–¥ model -—É—É–¥ –º”©–Ω —Å–∞–π–Ω –≥“Ø–π—Ü—ç—Ç–≥—ç–ª—Ç—ç–π –±”©–≥”©”©–¥ Multitask —Ö—É–≤–∏–ª–±–∞—Ä –Ω—å –Ω—ç–≥ model -–æ–æ—Ä –æ–ª–æ–Ω task –≥“Ø–π—Ü—ç—Ç–≥—ç—Ö –±–æ–ª–æ–º–∂—Ç–æ–π.

---
